# 技術検証結果

## 検証項目

[検証する技術的な課題を簡潔に記述]

例：
- FastAPIでの認証ライブラリ選択（FastAPI-Users vs Authlib vs 自前実装）
- React Server Components vs Client Componentsの選択
- データベースORMの選択（SQLAlchemy vs Tortoise ORM vs PonyORM）

---

## Context7調査サマリー

### 調査対象1: [ライブラリ/技術名A]

**調査手順**:
1. ライブラリIDを解決
   - libraryName: "[ライブラリ名]"
   - query: "[調査内容]"
   - 結果: `[libraryId]`（例: `/vercel/next.js`）

2. ドキュメント検索
   - libraryId: `[上記で取得したID]`
   - query: "[具体的な質問]"

**調査結果メモ**:
- [ポイント1]
- [ポイント2]
- [ポイント3]

**公式ドキュメントURL**:
- [URL1]
- [URL2]

---

### 調査対象2: [ライブラリ/技術名B]

**調査手順**:
1. ライブラリIDを解決
   - libraryName: "[ライブラリ名]"
   - query: "[調査内容]"
   - 結果: `[libraryId]`

2. ドキュメント検索
   - libraryId: `[上記で取得したID]`
   - query: "[具体的な質問]"

**調査結果メモ**:
- [ポイント1]
- [ポイント2]
- [ポイント3]

**公式ドキュメントURL**:
- [URL1]
- [URL2]

---

## 実験1: [アプローチA]

### 実験コード

**ファイル**: `spike/experiment-a.py`（または .js, .ts, .go等）

**注**: 以下のコード例はPythonを使用していますが、Node.js、Go、Rust等、任意の言語で実験できます。

**コード概要**:
```python
# 例: Python
# 最小限の実装コード（30-50行程度）
# 目的: [何を検証するか]

# 例：FastAPI-Usersの基本セットアップ
from fastapi import FastAPI
from fastapi_users import FastAPIUsers

app = FastAPI()
# ... セットアップコード
```

### 実験結果

- **セットアップ時間**: [X分]
- **コード行数**: [X行]
- **パフォーマンス**: [計測結果（例: ログイン処理 100ms）]
- **学習曲線**: [緩やか / 普通 / 急峻]
- **ドキュメント品質**: [豊富 / 普通 / 不足]

### 所感

[実験中に感じたこと、気づいた点を記述]

例：
- セットアップが非常に簡単で、公式ドキュメント通りに進められた
- カスタマイズの余地が少なく、特定のユースケースには向かない可能性
- エラーメッセージがわかりやすい

---

## 実験2: [アプローチB]

### 実験コード

**ファイル**: `spike/experiment-b.py`（または .js, .ts, .go等）

**コード概要**:
```python
# 例: Python
# 最小限の実装コード（30-50行程度）
# 目的: [何を検証するか]

# 例：Authlibの基本セットアップ
from fastapi import FastAPI
from authlib.integrations.starlette_client import OAuth

app = FastAPI()
# ... セットアップコード
```

### 実験結果

- **セットアップ時間**: [X分]
- **コード行数**: [X行]
- **パフォーマンス**: [計測結果]
- **学習曲線**: [緩やか / 普通 / 急峻]
- **ドキュメント品質**: [豊富 / 普通 / 不足]

### 所感

[実験中に感じたこと、気づいた点を記述]

例：
- セットアップに時間がかかり、ドキュメントが散在していた
- 柔軟性が高く、細かいカスタマイズが可能
- 学習コストが高いが、複雑な要件には対応しやすい

---

## 実験3: [アプローチC]（任意）

### 実験コード

**ファイル**: `spike/experiment-c.py`（または .js, .ts, .go等）

**コード概要**:
```python
# 例: Python
# 最小限の実装コード
```

### 実験結果

- **セットアップ時間**: [X分]
- **コード行数**: [X行]
- **パフォーマンス**: [計測結果]
- **学習曲線**: [緩やか / 普通 / 急峻]
- **ドキュメント品質**: [豊富 / 普通 / 不足]

### 所感

[実験中に感じたこと、気づいた点を記述]

---

## 定量比較

| 項目 | アプローチA | アプローチB | アプローチC（任意） |
|------|-------------|-------------|---------------------|
| セットアップ時間 | [X分] | [X分] | [X分] |
| コード行数 | [X行] | [X行] | [X行] |
| パフォーマンス | [X ms] | [X ms] | [X ms] |
| 学習曲線 | [緩やか/普通/急峻] | [緩やか/普通/急峻] | [緩やか/普通/急峻] |
| カスタマイズ性 | [低/中/高] | [低/中/高] | [低/中/高] |
| ドキュメント品質 | [豊富/普通/不足] | [豊富/普通/不足] | [豊富/普通/不足] |
| コミュニティ活動 | [活発/普通/停滞] | [活発/普通/停滞] | [活発/普通/停滞] |
| 保守性 | [高/中/低] | [高/中/低] | [高/中/低] |

---

## 定性評価

### アプローチA: [名前]

**長所**:
- [長所1]
- [長所2]
- [長所3]

**短所**:
- [短所1]
- [短所2]
- [短所3]

**適合するユースケース**:
- [ユースケース1]
- [ユースケース2]

---

### アプローチB: [名前]

**長所**:
- [長所1]
- [長所2]
- [長所3]

**短所**:
- [短所1]
- [短所2]
- [短所3]

**適合するユースケース**:
- [ユースケース1]
- [ユースケース2]

---

### アプローチC: [名前]（任意）

**長所**:
- [長所1]
- [長所2]

**短所**:
- [短所1]
- [短所2]

**適合するユースケース**:
- [ユースケース1]

---

## 推奨

### 採用アプローチ

**[アプローチ名]を採用**

### 理由

1. **[理由1]**: [詳細説明]

   例：パフォーマンス差は許容範囲内（5ms）で、セットアップ時間が3倍速い

2. **[理由2]**: [詳細説明]

   例：チームの学習コストを最小化でき、ドキュメントが豊富で問題解決が容易

3. **[理由3]**: [詳細説明]

   例：当面の要件（基本的なユーザー認証）には十分で、過剰な複雑性を避けられる

### トレードオフの許容

- **[トレードオフ1]**: [アプローチBの方が優れている点] → [なぜ許容できるか]

  例：カスタマイズ性はアプローチBの方が高いが、現時点で高度なカスタマイズは不要

- **[トレードオフ2]**: [アプローチBの方が優れている点] → [なぜ許容できるか]

  例：パフォーマンスはアプローチBの方が若干良いが、差は5msで体感できないレベル

---

## 将来的な考慮事項

### 要件変化時の対応

- **[シナリオ1]**: [要件が変わった場合]

  例：カスタマイズ要件が増えた場合 → アプローチBへの移行を検討

- **[シナリオ2]**: [要件が変わった場合]

  例：パフォーマンス要件が厳しくなった場合 → 最適化または別ライブラリ検討

### 技術的負債の回避

- [技術的負債になりうるポイント1]
- [技術的負債になりうるポイント2]

例：
- 採用したライブラリが非推奨になった場合の移行パスを確認済み
- 依存関係が少なく、将来的な置き換えが容易

---

## 参考資料

### Context7調査結果

- [ライブラリA公式ドキュメント URL]
- [ライブラリB公式ドキュメント URL]

### 外部記事・ブログ

- [参考記事1のタイトル](URL)
- [参考記事2のタイトル](URL)

### コミュニティ情報

- [GitHubリポジトリ](URL)
- [DiscordまたはSlack](URL)
- [Stack Overflow](URL)

---

## 次のステップ

このspike/results.mdを基に、以下を実施：

1. **design.md作成**: 技術検証結果を設計に反映（Step 0.9: Design）
2. **tasks.md更新**: 技術検証タスクを完了マーク
3. **spike/コード削除**: 実験コードは本番に含めない（spike/配下に隔離）

---

## カスタマイズのヒント

1. **定量データ重視**: 主観だけでなく、計測可能なデータを記録
2. **スクリーンショット**: 必要に応じて実験結果の画像を添付
3. **コード片保存**: spike/配下に実験コードを保存（Git管理可）
4. **ベンチマーク**: パフォーマンスが重要な場合、`spike/benchmark.py`を作成
5. **依存関係**: `spike/requirements.txt`（または`package.json`等）で依存を記録

## 例

具体的な記入例は [Step 0.5: Tech Spike](../workflows/step0.5-tech-spike.md) の「記録例」セクションを参照してください。
